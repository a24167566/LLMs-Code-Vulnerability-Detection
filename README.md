# LLMs-Code-Vulnerability-Detection
## Datasets
Datasets used for experiments an be found under the [datasets](https://github.com/a24167566/LLMs-Code-Vulnerability-Detection/tree/main/datasets) folder.

## Paper results
Experiment results an be found under the [paper_results](https://github.com/a24167566/LLMs-Code-Vulnerability-Detection/tree/main/paper_results) folder.

## Full context dataset and responses
To view and download the full context datasets and responses, please visit this [link](https://drive.google.com/drive/folders/13-NTXxKd6cEKr3jkK9SbXv6zRVgxoeaP?usp=share_link).

## Getting Started
### Prerequisites
Please ensure you have installed Python 3.11.7 or newer

### Environment setup
```
pip install -r requirements.txt
```

## Running the experiment
The format of the command line to run the experiment is as follows:
```
python3 ./llm_detection.py <GPT-MODEL> <PROMPT-TECH>
```

Replace `<GPT-MODEL>` with OpenAI supported models and `<PROMPT-TECH>` with one of the following:
* NONE (No strategy applied)
* ROLE
* COT
* APE-COT
* CO-STAR
* DYNAMIC

An example usage is as follows
```
python3 ./llm_detection.py gpt-3.5-turbo NONE
```