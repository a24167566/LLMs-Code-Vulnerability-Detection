# LLMs-Code-Vulnerability-Detection
This repo contains the source code and dataset for the paper **Detecting Code Vulnerabilities using LLMs**.

## Datasets
Datasets used for experiments an be found under the [datasets](https://github.com/a24167566/LLMs-Code-Vulnerability-Detection/tree/main/datasets) folder.

## Paper results
Experiment results an be found under the [paper_results](https://github.com/a24167566/LLMs-Code-Vulnerability-Detection/tree/main/paper_results) folder.

## Full context dataset and responses
To view and download the full context datasets and responses, please visit this [link](https://drive.google.com/drive/folders/13-NTXxKd6cEKr3jkK9SbXv6zRVgxoeaP?usp=share_link).

## Getting Started
### Prerequisites
Please ensure you have installed Python 3.11.7 or newer and set up your OenAI API key.

Download the full context datasets and place them under the datasets folder. See instructions at [Full context dataset and responses](#full-context-dataset-and-responses).

### Environment setup
```
pip install -r requirements.txt
```

## Running the experiment
The format of the command line to run the experiment is as follows:
```
python3 ./llm_detection.py <GPT-MODEL> <PROMPT-ENG-TECH>
```

Replace `<GPT-MODEL>` with OpenAI supported models and `<PROMPT-ENG-TECH>` with one of the following:
* NONE (No strategy applied)
* ROLE
* COT
* APE-COT
* CO-STAR
* DYNAMIC

An example usage is as follows
```
python3 ./llm_detection.py gpt-3.5-turbo NONE
```
